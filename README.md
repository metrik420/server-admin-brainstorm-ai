# ğŸš€ ServerAI Knowledge Engine - Complete Docker Setup

## One-Command Deployment

```bash
# Clone or download files to a directory, then:
chmod +x setup.sh && ./setup.sh
```

Your complete ServerAI Knowledge Engine will be running at **http://localhost:420420**

## ğŸ—ï¸ Architecture

This is a complete microservices setup with:

- **Frontend** (React/Vite): Web interface on port 420420
- **Backend** (Python/FastAPI): REST API on port 420421  
- **Database** (PostgreSQL): Data storage
- **Cache** (Redis): Task queue and caching
- **Worker** (Celery): Background crawling tasks
- **Nginx**: Load balancer and reverse proxy

## ğŸ“ Complete File Structure

```
serverai-engine/
â”œâ”€â”€ docker-compose.yml          # Main orchestration
â”œâ”€â”€ setup.sh                   # One-command setup
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ nginx.conf
â”‚   â””â”€â”€ (React app files)
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ Dockerfile.worker
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ main.py                 # FastAPI app
â”‚   â”œâ”€â”€ crawler.py              # Web crawler
â”‚   â”œâ”€â”€ database.py             # Database layer
â”‚   â”œâ”€â”€ models.py               # Data models
â”‚   â””â”€â”€ worker.py               # Background tasks
â”œâ”€â”€ nginx/
â”‚   â””â”€â”€ nginx.conf              # Load balancer config
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql                # Database schema
â””â”€â”€ config/
    â””â”€â”€ crawler.yml             # Crawler configuration
```

## ğŸ”§ Configuration

### Environment Variables
- `DATABASE_URL`: PostgreSQL connection
- `REDIS_URL`: Redis connection
- `STORAGE_PATH`: File storage location

### Crawler Settings
Edit `config/crawler.yml` to:
- Add more crawl targets
- Modify topic classifications
- Adjust crawler behavior

## ğŸ“Š Features

### Web Interface
- Real-time dashboard with stats
- Topic-based content organization
- Crawler status monitoring
- Search functionality

### Backend API
- RESTful API with FastAPI
- Real-time crawler control
- Content classification
- Full-text search

### Data Storage
- PostgreSQL for structured data
- File system for organized content
- Redis for task queuing
- Persistent Docker volumes

## ğŸ›ï¸ Management Commands

```bash
# View all services
docker-compose ps

# View logs
docker-compose logs -f [service-name]

# Restart specific service
docker-compose restart [service-name]

# Scale workers
docker-compose up -d --scale crawler-worker=4

# Database access
docker-compose exec postgres psql -U serverai -d serverai_db

# Redis access
docker-compose exec redis redis-cli

# File system access
docker-compose exec backend bash
```

## ğŸ“ˆ Scaling

To handle more load:

```bash
# Scale crawler workers
docker-compose up -d --scale crawler-worker=4

# Add more backend instances
docker-compose up -d --scale backend=2

# Update nginx config for load balancing
```

## ğŸ”’ Security

For production:
- Change default passwords in docker-compose.yml
- Add SSL certificates
- Configure firewall rules
- Enable authentication
- Use Docker secrets

## ğŸš¨ Troubleshooting

```bash
# Check service health
curl http://localhost:420420/health

# View API docs
open http://localhost:420421/docs

# Reset everything
docker-compose down -v && docker-compose up --build -d
```

## ğŸ“¦ Storage Volumes

Data persists in Docker volumes:
- `postgres_data`: Database storage
- `redis_data`: Cache and queue data
- `crawler_storage`: Organized content files

## ğŸ”„ Updates

```bash
# Pull latest images
docker-compose pull

# Rebuild and restart
docker-compose up --build -d
```

This setup provides a complete, production-ready knowledge crawling system that's easy to deploy and scale!